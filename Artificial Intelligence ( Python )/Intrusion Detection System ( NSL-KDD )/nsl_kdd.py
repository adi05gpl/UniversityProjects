# -*- coding: utf-8 -*-
"""NSL-KDD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ItviSZDbsYd7zcPEUS-5PVOLv576oPS-

# Explorarea setului de date NSL-KDD

Scopul acestui caiet este o explorare de bază a setului de date NSL-KDD. Iată care sunt obiectivele acestei explorări:
* Obținerea unei înțelegeri de bază a setului de date
* Analizați modul în care setul de date ar putea fi utilizat pentru a prezice anomaliile sau atacurile din rețea
* Parcurgerea câtorva concepte fundamentale de construire a modelelor de învățare automată

De-a lungul timpului, vom efectua manual unele lucrări care ar putea fi realizate în moduri mai eficiente folosind funcționalitatea oferită în sci-kit. Intenția aici este de a fi mai deliberat cu privire la procesul de înțelegere a ceea ce facem și de ce. Vom examina modul de abordare a unora dintre aceste probleme folosind instrumentele încorporate într-un caiet ulterior.
"""

# module imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
import random

# model imports
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

# processing imports
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

"""# Extragerea datelor
Vom începe prin a prelua setul nostru de date. Există câteva opțiuni pentru seturile de date aici, așa că vom construi câteva căi și vom folosi comentariile pentru a le alege pe cele pe care le dorim.
"""

# încărcare citire a fișierelor
file_path_20_percent = 'KDDTrain+_20Percent.txt'  #folosirea datelor pentru antrenare
file_path_full_training_set = 'KDDTrain+.txt'     #folosirea datelor pentru antrenare
file_path_test = 'KDDTest+.txt'                   #folosirea datelor pentru testare

df = pd.read_csv(file_path_full_training_set)
test_df = pd.read_csv(file_path_test)

"""Setul de date nu include nume de coloane, așa că haideți să le adăugăm."""

# add the column labels
columns = (['duration'
,'protocol_type'
,'service'
,'flag'
,'src_bytes'
,'dst_bytes'
,'land'
,'wrong_fragment'
,'urgent'
,'hot'
,'num_failed_logins'
,'logged_in'
,'num_compromised'
,'root_shell'
,'su_attempted'
,'num_root'
,'num_file_creations'
,'num_shells'
,'num_access_files'
,'num_outbound_cmds'
,'is_host_login'
,'is_guest_login'
,'count'
,'srv_count'
,'serror_rate'
,'srv_serror_rate'
,'rerror_rate'
,'srv_rerror_rate'
,'same_srv_rate'
,'diff_srv_rate'
,'srv_diff_host_rate'
,'dst_host_count'
,'dst_host_srv_count'
,'dst_host_same_srv_rate'
,'dst_host_diff_srv_rate'
,'dst_host_same_src_port_rate'
,'dst_host_srv_diff_host_rate'
,'dst_host_serror_rate'
,'dst_host_srv_serror_rate'
,'dst_host_rerror_rate'
,'dst_host_srv_rerror_rate'
,'attack'
,'level'])

df.columns = columns
test_df.columns = columns

# verificare numele coloanelor adaugate
df.head()

"""# Transformări de date
Primele transformări pe care vom dori să le facem sunt în jurul câmpului de atac. Vom începe prin a adăuga o coloană care codifică valorile "normale" ca 0 și orice altă valoare ca 1. Vom folosi această coloană ca clasificator pentru un model binar simplu care identifică orice atac.
"""

# map normal la 0, toate atacurile la 1
is_attack = df.attack.map(lambda a: 0 if a == 'normal' else 1)
test_attack = test_df.attack.map(lambda a: 0 if a == 'normal' else 1)

df['attack_flag'] = is_attack
test_df['attack_flag'] = test_attack

# rezultate
df.head()

np.shape(df)

set(df['protocol_type'])

set(df['attack'])

set(df['service'])

# liste pentru a păstra clasificările atacurilor noastre
dos_attacks = ['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm']
probe_attacks = ['ipsweep','mscan','nmap','portsweep','saint','satan']
privilege_attacks = ['buffer_overflow','loadmdoule','perl','ps','rootkit','sqlattack','xterm']
access_attacks = ['ftp_write','guess_passwd','http_tunnel','imap','multihop','named','phf','sendmail','snmpgetattack','snmpguess','spy','warezclient','warezmaster','xclock','xsnoop']

# le vom folosi mai jos pentru trasare
attack_labels = ['Normal','DoS','Probe','Privilege','Access']

# funcție ajutătoare pentru a trece la cartografierea cadrului de date
def map_attack(attack):
    if attack in dos_attacks:
        # dos_attacks map to 1
        attack_type = 1
    elif attack in probe_attacks:
        # probe_attacks mapt to 2
        attack_type = 2
    elif attack in privilege_attacks:
        # Atacurile de escaladare a privilegiilor se referă la 3
        attack_type = 3
    elif attack in access_attacks:
        # atacurile de acces de la distanță se mapează la 4
        attack_type = 4
    else:
        # normalul este la 0
        attack_type = 0

    return attack_type

# mapează datele și se alătură setului de date
attack_map = df.attack.apply(map_attack)
df['attack_map'] = attack_map

test_attack_map = test_df.attack.apply(map_attack)
test_df['attack_map'] = test_attack_map

# Rezultate
df.head()

set(df['attack_map'])

"""Este evident că durata conexiunii și datele conținute spun multe despre dacă este un atac sau un trafic normal.

# Feature engineering

Construim caracteristici. protocol_type, service și flag sunt de interes. Există suficiente variații între acestea pentru a putea obține un nivel de identificare de bază.Se  adaug[ câteva date numerice de bază: duration, src_bytes, dst_bytes. datele acestea se primesc de la echipament (in teorie, adica din baze de date)
"""

# obține setul inițial de caracteristici codificate și le codifică
features_to_encode = ['protocol_type', 'service', 'flag']
encoded = pd.get_dummies(df[features_to_encode])
test_encoded_base = pd.get_dummies(test_df[features_to_encode])

# nu toate caracteristicile se regăsesc în setul de testare, așa că trebuie să ținem cont de diferențe
test_index = np.arange(len(test_df.index))
column_diffs = list(set(encoded.columns.values)-set(test_encoded_base.columns.values))

diff_df = pd.DataFrame(0, index=test_index, columns=column_diffs)

# va trebui, de asemenea, să reordonăm coloanele pentru a se potrivi, așa că să le luăm pe acelea
column_order = encoded.columns.to_list()

# se adaugă noile coloane
test_encoded_temp = test_encoded_base.join(diff_df)

# reordonează coloanele
test_final = test_encoded_temp[column_order].fillna(0)

# get numeric features, nu ne vom preocupa de codificarea acestora în acest moment
numeric_features = ['duration', 'src_bytes', 'dst_bytes']

# model to fit/test
to_fit = encoded.join(df[numeric_features])
test_set = test_final.join(test_df[numeric_features])

"""Merită să atragem atenția asupra câtorva lucruri aici. În primul rând, `pd.get_dummies` este o metodă care ne permite să efectuăm o codificare rapidă și la cald a coloanelor noastre. Aceasta ia fiecare valoare pe care o găsește într-o singură coloană și creează o coloană individuală pentru fiecare valoare, cu un `0` sau `1` indicând dacă acea coloană este "hot".

Un lucru pe care îl constatăm este că nu orice valoare se regăsește în datele de test. Astfel, se creează diferite forme ale cadrului nostru de date. De aceea, am adăugat câteva coloane, le-am completat și le-am reordonat. Știm că sunt toate zerouri, deoarece nu se află în date.

Acum să mergem mai departe și să ne stabilim obiectivele de clasificare. Pentru început, vom face ambele seturi de antrenament: clasificări binare și multiple.
"""

# creați clasificările noastre țintă
binary_y = df['attack_flag']
multi_y = df['attack_map']

test_binary_y = test_df['attack_flag']
test_multi_y = test_df['attack_map']

# construim seturile de antrenament
binary_train_X, binary_val_X, binary_train_y, binary_val_y = train_test_split(to_fit, binary_y, test_size=0.6)
multi_train_X, multi_val_X, multi_train_y, multi_val_y = train_test_split(to_fit, multi_y, test_size = 0.6)

binary_train_X.info()

binary_train_X.sample(5)

"""# Model fitting
KNN
"""

# model pentru clasificarea binară
binary_model = KNeighborsClassifier()
binary_model.fit(binary_train_X, binary_train_y)
binary_predictions = binary_model.predict(binary_val_X)

# calculează și afișează precizia noastră de bază
base_rf_score = accuracy_score(binary_predictions,binary_val_y)
base_rf_score

# definim lista de modele pe care dorim să le testăm
models = [
    LogisticRegression(max_iter=250),
    KNeighborsClassifier(),
]

# o listă goală pentru a surprinde performanța fiecărui model
model_comps = []

# trecem prin modele și populăm lista noastră
for model in models:
    model_name = model.__class__.__name__
    accuracies = cross_val_score(model, binary_train_X, binary_train_y, scoring='accuracy')
    for count, accuracy in enumerate(accuracies):
        model_comps.append((model_name, count, accuracy))

# o diagramă cu casete va fi bună pentru a ne arăta performanța generală și variația modelelor.
result_df = pd.DataFrame(model_comps, columns=['model_name', 'count', 'accuracy'])
result_df.pivot(index='count',columns='model_name',values='accuracy').boxplot(rot=45)

# model mulit classification
multi_model = model.fit(multi_train_X, multi_train_y)
multi_predictions = multi_model.predict(multi_val_X)

# scor
accuracy_score(multi_predictions,multi_val_y)